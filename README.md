# LLM_Inference_on_Edge
This project evaluates the performance of advanced language models, specifically NanoLLM and Ollama, on NVIDIA's Jetson AGX Orin for edge computing. It focuses on their operational viability, comparing efficiency metrics like inference speed, memory usage, and power consumption against GPT-2 as a baseline.
